"""
03_returns_calculation.py

This module computes log-returns for a panel dataset containing multiple equity
tickers. Log-returns are calculated independently for each asset to preserve
temporal integrity and avoid cross-sectional distortions. The script imports:

    data/train_prices.csv
    data/validation_prices.csv

and outputs:

    data/train_returns.csv
    data/validation_returns.csv

The returns are computed using the "Adj Close" column, which is the standard
practice for financial series analysis, as it accounts for corporate actions
such as stock splits and dividends.
"""

import pandas as pd
import numpy as np
import os
from typing import Tuple

def compute_log_returns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute log-returns for a panel dataset grouped by ticker.

    Academic notes:
    ---------------
    Log-returns are preferred in financial econometrics due to their
    time-additivity, approximate normality, and scale invariance 
    (Campbell, Lo & MacKinlay, 1997). Computing returns separately for
    each ticker is essential because assets differ in listing dates and
    data availability, making pooled differencing invalid.

    The `.transform()` method is used within the groupby operation to ensure 
    that the resulting series of returns is perfectly aligned with the original 
    DataFrame's index, avoiding structural alignment errors and guaranteeing 
    correct mapping of returns back to the panel structure.

    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame containing columns ["Date", "Ticker", "Adj Close"].
        Assumed to be pre-sorted chronologically within each Ticker group.

    Returns
    -------
    returns_df : pd.DataFrame
        Dataset including columns ["Date", "Ticker", "Adj Close", "LogReturn"].
    """

    # Ensure temporal order within each asset class (Ticker)
    df = df.sort_values(["Ticker", "Date"])

    print(f"Applying log-return transformation to {df['Ticker'].nunique()} unique assets...")

    # Compute log-return for each ticker (panel structure) using .transform()
    # LogReturn = ln(P_t / P_{t-1}) = ln(P_t) - ln(P_{t-1})
    df["LogReturn"] = df.groupby("Ticker")["Adj Close"].transform(
        lambda x: np.log(x / x.shift(1))
    )
    
    # Progress Print: Show a sample of the first calculated returns
    print("\n[PROGRESS] Sample of calculated log-returns:")
    print(df.head(5))

    # Remove rows where log-return is NaN (first observation per ticker)
    # This addresses the initial NaN generated by the differencing operation (x.shift(1)).
    df = df.dropna(subset=["LogReturn"]).reset_index(drop=True)

    return df


def validate_returns_dataset(df: pd.DataFrame, set_name: str):
    """
    Performs a final validation check for NaN values in the LogReturn column.
    """
    nan_count = df["LogReturn"].isna().sum()
    if nan_count == 0:
        print(f"\n[VALIDATION SUCCESS] {set_name} dataset passed integrity check: No NaN values found in 'LogReturn'.")
    else:
        print(f"\n[VALIDATION FAILED] {set_name} dataset contains {nan_count} NaN values in 'LogReturn'.")


# ---------------------------------------------------------------------
# MAIN EXECUTION
# ---------------------------------------------------------------------

if __name__ == "__main__":
    # --- 1. Define Input/Output Paths ---
    
    # Assuming input price files are located in a standard 'data' directory
    train_path = "data/train_prices.csv"
    val_path = "data/validation_prices.csv"
    
    # Define output directory and file names for the resulting returns
    output_dir = "data"
    train_output_path = os.path.join(output_dir, "train_returns.csv")
    val_output_path = os.path.join(output_dir, "validation_returns.csv")

    print("Starting Log-Return Calculation Pipeline...")
    
    # --- 2. Load Data ---
    print(f"\n[STEP 1] Loading Training data from: {train_path}")
    train_df = pd.read_csv(train_path, parse_dates=["Date"])
    
    print(f"[STEP 1] Loading Validation data from: {val_path}")
    val_df = pd.read_csv(val_path, parse_dates=["Date"])

    # --- 3. Compute Log-Returns ---
    
    print("\n[STEP 2] Computing Log-Returns for Training Set...")
    train_returns = compute_log_returns(train_df)
    
    print("\n[STEP 2] Computing Log-Returns for Validation Set...")
    val_returns = compute_log_returns(val_df)
    
    # --- 4. Validation Check ---
    validate_returns_dataset(train_returns, "Training Set")
    validate_returns_dataset(val_returns, "Validation Set")

    # --- 5. Save Results ---
    
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    print("\n[STEP 3] Saving calculated returns to CSV files...")
    
    train_returns.to_csv(train_output_path, index=False)
    val_returns.to_csv(val_output_path, index=False)

    # --- Final Confirmation ---
    print("\n=======================================================")
    print("SUCCESS: Log-returns datasets generated and ready for modeling.")
    print(f"→ Training Returns saved to: {train_output_path}")
    print(f"→ Validation Returns saved to: {val_output_path}")
    print("The new datasets include 'LogReturn' and are clean (no initial NaN values).")
    print("=======================================================")